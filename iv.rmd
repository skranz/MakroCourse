#< ignore
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error=TRUE)
```
#>

#. include slide_defaults.rmd

#< settings
libs: ggplot2
#>

#< css
@media print {
  .slide-container-div {
    page-break-inside: auto;
  }
}

.border-table td {
  border: 1px solid grey;
  padding-right: 4px;
  padding-left: 4px;
}

#>


#. frame
<center>
<h2> Thoughts on Causality, Endogeniety and Instrumental Variable Estimation </h2>

<h3> Sebastian Kranz </h3>
<h3> Uni Ulm</h3>
</center>

#. section Estimating A Demand Function

#. frame Estimating A Demand Function

+ As leading example, we consider the task of estimating a demand function using past observations of prices and quantities (and costs).

+ To simplify thinks, we assume the observed data has been generated by a simple true data generating proceess.

#. frame DGP: Demand Function

+ In period $t$ we have a simple linear demand function of the form:

  $$D_t(p) = a_t - b p$$

  with an intercept $a_t$ given by
  
  $$a_t = a_0 + u_t$$
  
+ The slope $b$ and $a_0$ are some fixed numbers which are the same for all periods.

+ $u_t$ is a i.i.d. normaly distributed random variable with mean 0. We call $u_t$ a demand shock, since it shifts the demand function up or down.

#. frame DGP: Price Setting

+ Assume the observed prices $p_t$ has been set by a clever decision maker who knew, $b$, $a_0$ and the demand shock $u_t$ and wanted to maximize the following static profit function:

  $$
  \begin{align*}
  \pi =& D_t(p_t)(p_t-c_t) \\
     =& (a_0 - b p + u_t)(p_t-c_t)
  \end{align*}
  $$

  + $c_t$ shall be the constant variable cost of the product that can change between periods.
  
+ Solving the first order condition $\pi'(p_t) = 0$ yields the following price in period $t$:

  $$p_t = \frac{a_0+u_t}{2*b} + \frac{c_t}{2}$$
  
  and a resulting output $q_t = D_t(p_t)$ of

  $$q_t = \frac{a_0+u_t}{2} - b \frac{c_t}{2}$$

#. frame DGP: Simulation

+ If we specify some concrete numbers for $a_0$, $b$, the standard deviation of $u$ and a distribution of costs $c_t$, we can write a short R program that simulates data from this DGP.

```{r precompute=TRUE, preknit=TRUE}
set.seed(123456)
a0 = 100 # average intercept of demand function
b = 1 # -b is the slope of the demand function
T = 200 # number of observations
sigma.eps = 5 # standard deviation of demand shock

# Draw T demand shocks
eps = rnorm(T,0,sigma.eps)

# Draw random costs
c = runif(T, 10, 15)

# Profit maximizing price
p = (a0+eps) / (2*b) + c / 2

# Demand at p
q = (a0+eps) - b * p

# Combine data into a data frame
dat = data.frame(t=1:T,p=p,q=q,c=c)

# Show first 3 rows
head(dat,3)
```
```{r}
.whiskers = nlist(a0,b,T)
```


#. frame Plot of the simulated data
Here is a plot of the prices vs sold quantity:
```{r preknit=TRUE}
library(ggplot2)
ggplot(dat, aes(x=p, y=q)) + 
  geom_point()+
  geom_smooth(method=lm, se=FALSE)
```

#. frame A simple OLS regression
+ The blue line shows the fitted values from a simple linear regression of $q$ on $p$
+ Recall that the average intercept of the demand function is $a_0 = {{a_0}}$ and the slope is $-b = {{-b}}$.
+ An OLS regression of $q$ on $p$ yields estimated coefficients that are completely different:
```{r preknit=TRUE}
summary(lm(q~p))
```
+ As you can see from the small standard errors, this difference is **not** due to a small sample estimation error. Even if the number of observations $T$ grows large, the estimated coefficients of the regression line won't converge towards the intercept and slope of the demand function.

#. frame Best Linear Predictor and OLS

+ Given a dependent random variable $y$ and random or deterministic explanatory variables $x=(x_1,...,x_K)$, the predicted value $\hat{y}(\beta | x)$ for a linear predictor $\beta = (\beta0, beta_1, $ is defined by
$$\hat{y}(\beta | x) = \beta_0 + \beta_1 x_1 + ... + \beta_K x_K$$

+ The best linear predictor minimizes the expected squared differences between $y$ and the predicted value $\hat{y(\beta | x)}$, i.e.

  $$\beta^{BLP} = \arg \min_{\beta} E (y-\hat{y}(\beta | x))^2 $$

  + The expectation is taken over the joint distribution of $y$ and all $x$.

+ The OLS estimator is simply defined as the sample analogue to the best linear predictor:

$$\hat{\beta} = \arg \min_{\beta} \sum_{t=1}^T {(y_t-\hat{y}(\beta | x_t))^2} $$
  where here $x_t$ denotes the vector of explanatory variables in period $t$.

+ Under fairly weak conditions, the OLS estimator $\hat{\beta}$ converges in probability to the best linear predictor $\beta^{BLP}$.

#. frame Best Linear Predictor

+ The relationship between random variables $y$ and $x_1,...x_K$ can always be written in the following form:

  $$y = \beta_0^{BLP} + \beta_1^{BLP} x_1 + ... + \beta_K^{BLP} x_K + \eta$$

  such that the `error term` $\eta = y - \bar($
    + has mean 0: $E(\eta) = 0$ and
    + is not correlated with any explanatory variable $\cor(\eta, x_k) = 0$$ for all $k=1,...,K$.

+ This result always holds, even if the "true" relationship between $y$ and the $x$ has a much more complicated, non-linear form.     

##< proof
Ommited
##>

#. frame Remark: OLS Residuals

+ A similar result holds for the OLS residuals.
+ Let $\hat{\beta}$ be the OLS estimates and $\hat{\eta}$ the residuals:

  $$y = \hat{\beta_0} + \hat{\beta_1} x_1 + ... + \hat{\beta_K} x_K + \hat{\eta}$$

  The OLS residuals $\hat{\eta}$
    + have mean 0: $\sum_{t=1}^T{\hat{\eta}} = 0$ and
    + are not correlated with any explanatory variable $\cor(\hat{\eta}, x_k) = 0$$ for all $k=1,...,K$.

#. frame Best linear predictors in our example

+ Recall the OLS estimates of the regression of output on prices in our example:
```{r preknit=TRUE}
lm(q~p, data=dat)
```



#. frame BLP in our example

```{r, preknit=TRUE, precompute=TRUE}
sim.data = function(T=200, a0=100, b=1, sigma.u=5, cmin=10, cmax=15) {
  # Draw T demand shocks
  u = rnorm(T,0,sigma.u)
  
  # Draw random costs
  c = runif(T, cmin, cmax)
  
  # Profit maximizing price
  p = (a0+eps) / (2*b) + c / 2
  
  # Demand at p
  q = (a0+eps) - b * p
  
  # Combine data into a data frame
  dat = data.frame(t=1:T,p=p,q=q,c=c, u=u)
  dat
}
dat = sim.data(T=10000)
summary(lm(q~p, data=dat))
```

#. section Non-Linearities

#. Linearized Causal Effects


```{r}
T = 1000
sigma.eps = 0.1
eps = rnorm(T,0,sigma.eps)
xnorm = function(x, factor=5) {x * factor*sd(eps) / sd(x)}
y.fun = function(x) ifelse(x > 1, 1, x) + eps

x.min = 5; x.max = 20

x.vals = c(x.min,x.max)

x.r1 = xnorm(runif(T,0,2))
x.r2 = xnorm(sample(x.vals,T,replace=TRUE))

x.e1 = xnorm(x.r1 - 5*eps)
x.e2 = xnorm(x.r2 - eps)


# for x<1 slope = 1, else slope = 0
y.r1 = y.fun(x.r1)
y.r2 = y.fun(x.r2)
y.e1 = y.fun(x.e1)
y.e2 = y.fun(x.e2)



plot(x.r1,y.r1, xlim=range(c(x.r1,x.e1)),ylim=range(c(y.r1,y.e1)))
points(x.e1,y.e1, col="red")

lm(y.e1 ~ x.e1)
cor(x.e1, eps)
cor(y.e1, eps)

```

```{r}
T = 1000 # number of observations
beta0 = 0 # average intercept of demand function
beta1 = -1 # -b is the slope of the demand function
sigma.eps = 5 # standard deviation of demand shock

# Draw T demand shocks
eps = rnorm(T,0,sigma.eps)

# Draw random costs
c = runif(T, 10, 15)

# Profit maximizing price
p = (a0+eps) / (2*b) + c / 2

# Endogenous price
p = (eps +runif(T,-1,1)) / 2
# random price
#p = (runif(T,-1,1)) / 2


# Demand at p
q = beta0 + beta1 * p + eps
q = beta0 + ifelse(p>0,0,beta1 * p) + eps


plot(p,q)

```


#. frame Non-Linearities
```{r}
T = 10000
a0 = 100 # average intercept of demand function
b = 1 # -b is the slope of the demand function
sigma.eps = 4 # standard deviation of demand shock

# Draw T demand shocks
eps = round(rnorm(T,0,sigma.eps),2)

# Draw random costs
c = runif(T, 10, 25)
c2 = c^2

# Endogenous price
p = (a0+eps) / (2*b) + c / 2

#p = c * runif(T,1,1.1)
p2 = p^2

# Demand at p
q = (a0+eps) -  p - 0.1*p2
beta.p = c(-1,-0.1)


summary(ols <- lm(q~p+p2))
library(AER)
summary(iv <- ivreg(q~p+p2 | c+c2))

mfx = function(p, beta) {
  beta[1] + 2*beta[2]*p
}
p.seq = seq(min(p),max(p), length=101)
mfx.true = mfx(p.seq,beta.p )
mfx.iv = mfx(p.seq, coef(iv)[2:3])
mfx.ols = mfx(p.seq, coef(ols)[2:3])

plot(p.seq, mfx.true, type="l", ylim=range(c(mfx.true,mfx.ols,mfx.iv)))
lines(p.seq, mfx.iv, col="red")
lines(p.seq, mfx.ols, col="blue")

```


#. frame Two endogenous variables

```{r}
T = 100000
sigma.eps = 2
eps = rnorm(T,0,sigma.eps)

xnorm = function(x, factor=4) {x * factor*sd(eps) / sd(x)}
z1 = rnorm(T,0,1)
z2 = rnorm(T,0,1)+0.1*z1
x1 = rnorm(T,0,2)+z1+0.1*z2+eps
x2 = rnorm(T,0,2)-0.5*z1-z2+0.1*x1+2*eps
cor(x1,x2)

y = 0 + x1 + x2 + eps
# biassed OLS
lm(y~x1+x2)

# first stage
summary(reg11 <- lm(x1~z1+z2))
summary(reg12 <- lm(x2~z1+z2))

eta1 = resid(reg11)
eta2 = resid(reg12)
x1.hat = fitted(reg11)
x2.hat = fitted(reg12)
cor(eta1, eta2)
cor(x1.hat, x2.hat)
cor(x1.hat, eta1)
cor(x1.hat, eta2) # = 0 why?
cor(x2.hat, eta1)
cor(x2.hat, eta2) # = 0 why?

# Another simulation
x1 = rnorm(T)
x2 = -0.5*x1 + rnorm(T)
eps1 = rnorm(T)
eps2 = rnorm(T) + 10*eps1

y1 = x1 + x2 + eps1
y2 = -0.5*x1 + 2*x2 + eps2
cor(y1,y2)

summary(reg11 <- lm(y1~x1+x2))
summary(reg12 <- lm(y2~x1+x2))

eta1 = resid(reg11)
eta2 = resid(reg12)



y1.hat = fitted(reg11)
y2.hat = fitted(reg12)
cor(eta1, eta2)
cor(y1.hat, y2.hat)
cor(y1.hat, eta1)
cor(y1.hat, eta2) # = 0 why?
cor(y2.hat, eta1)
cor(y2.hat, eta2) # = 0 why?

```

